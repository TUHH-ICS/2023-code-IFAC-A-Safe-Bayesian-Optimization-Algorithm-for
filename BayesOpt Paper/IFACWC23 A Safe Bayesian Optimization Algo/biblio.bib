
@INPROCEEDINGS{smgo,
author={Sabug, Lorenzo and Ruiz, Fredy and Fagiano, Lorenzo}, 
booktitle={2021 60th IEEE Conference on Decision and Control (CDC)},
title={Trading-off safety, exploration, and exploitation in learning-based optimization: a Set Membership approach},   
year={2021},  
volume={},  
number={},  
pages={1462-1467},  
doi={10.1109/CDC45484.2021.9683334}}

@article{nelder_mead,
author = {Lagarias, Jeffrey C. and Reeds, James A. and Wright, Margaret H. and Wright, Paul E.},
title = {Convergence Properties of the Nelder--Mead Simplex Method in Low Dimensions},
journal = {SIAM Journal on Optimization},
volume = {9},
number = {1},
pages = {112-147},
year = {1998},
doi = {10.1137/S1052623496303470},

URL = { 
        https://doi.org/10.1137/S1052623496303470
    
},
eprint = { 
        https://doi.org/10.1137/S1052623496303470
    
}
,
    abstract = { The Nelder--Mead simplex algorithm, first published in 1965, is an enormously popular direct search method for multidimensional unconstrained minimization. Despite its widespread use, essentially no theoretical results have been proved explicitly for the Nelder--Mead algorithm. This paper presents convergence properties of the Nelder--Mead algorithm applied to strictly convex functions in dimensions 1 and 2. We prove convergence to a minimizer for dimension 1, and various limited convergence results for dimension 2. A counterexample of McKinnon gives a family of strictly convex functions in two dimensions and a set of initial conditions for which the Nelder--Mead algorithm converges to a nonminimizer. It is not yet known whether the Nelder--Mead method can be proved to converge to a minimizer for a more specialized class of convex functions in two dimensions. }
}



@article{euxfel_pulses,
author = {Gorel, Alexander and Grünbein, Marie and Bean, Richard and Bielecki, Johan and Hilpert, Mario and Cascella, Michele and Colletier, Jacques-Philippe and Fangohr, H. and Foucar, Lutz and Hartmann, Elisabeth and Hunter, Mark and Kirkwood, Henry and Kloos, Marco and Letrun, Romain and Michelat, Thomas and Shoeman, Robert and Sztuk-Dambietz, Jolanta and Tetreau, Guillaume and Zimmermann, Herbert and Schlichting, Ilme},
year = {2020},
month = {12},
pages = {1145},
title = {Shock Damage Analysis in Serial Femtosecond Crystallography Data Collected at MHz X-ray Free-Electron Lasers},
volume = {10},
journal = {Crystals},
doi = {10.3390/cryst10121145}
}

@book{scholkopf2002learning,
  title={Learning with kernels: support vector machines, regularization, optimization, and beyond},
  author={Sch{\"o}lkopf, Bernhard and Smola, Alexander J and Bach, Francis and others},
  year={2002},
  publisher={MIT press}
}

@article{bull2011convergence,
  title={Convergence rates of efficient global optimization algorithms.},
  author={Bull, Adam D},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={10},
  year={2011}
}

@article{srinivas2009gaussian,
  title={Gaussian process optimization in the bandit setting: No regret and experimental design},
  author={Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M and Seeger, Matthias},
  journal={arXiv preprint arXiv:0912.3995},
  year={2009}
}

@misc{kanagawa2017,
  doi = {10.48550/ARXIV.1709.00147},
  
  url = {https://arxiv.org/abs/1709.00147},
  
  author = {Kanagawa, Motonobu and Sriperumbudur, Bharath K. and Fukumizu, Kenji},
  
  keywords = {Numerical Analysis (math.NA), Machine Learning (stat.ML), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences, 65D30 (Primary), 65D32, 65D05, 46E35, 46E22 (Secondary)},
  
  title = {Convergence Analysis of Deterministic Kernel-Based Quadrature Rules in Misspecified Settings},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{femtosecond_seb,
author = {Schulz, S and Grguras, Ivanka and Behrens, C and Bromberger, Hubertus and Costello, John and Czwalinna, Marie and Felber, Matthias and Hoffmann, M and Ilchen, M and Liu, H and Mazza, Tommaso and Meyer, M and Pfeiffer, Sven and Predki, Pawel and Schefer, S and Schmidt, Christian and Wegner, U and Schlarb, H. and Cavalieri, A},
year = {2015},
month = {01},
pages = {5938},
title = {Femtosecond all-optical synchronization of an X-ray free-electron laser},
volume = {6},
journal = {Nature communications},
doi = {10.1038/ncomms6938}
}

@article{direct,
author = {Jones, Donald and Perttunen, C. and Stuckman, B.},
year = {1993},
month = {01},
pages = {157-181},
title = {Lipschitzian Optimisation Without the Lipschitz Constant},
volume = {79},
journal = {Journal of Optimization Theory and Applications},
doi = {10.1007/BF00941892}
}

@INPROCEEDINGS{Svensson,  author={Svensson, Andreas and Dahlin, Johan and Schön, Thomas B.},  booktitle={2015 IEEE 6th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)},   title={Marginalizing Gaussian process hyperparameters using sequential Monte Carlo},   year={2015},  volume={},  number={},  pages={477-480},  doi={10.1109/CAMSAP.2015.7383840}}

@ARTICLE{MacKey,  author={MacKay, David J. C.},  journal={Neural Computation},   title={Comparison of Approximate Methods for Handling Hyperparameters},   year={1999},  volume={11},  number={5},  pages={1035-1068},  doi={10.1162/089976699300016331}}


@InProceedings{lalchand20a,
  title =    { Approximate Inference for Fully Bayesian Gaussian Process Regression },
  author =       {Lalchand, Vidhi and Rasmussen, Carl Edward},
  booktitle =    {Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference},
  pages =    {1--12},
  year =   {2020},
  editor =   {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
  volume =   {118},
  series =   {Proceedings of Machine Learning Research},
  month =    {08 Dec},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v118/lalchand20a/lalchand20a.pdf},
  url =    {https://proceedings.mlr.press/v118/lalchand20a.html},
  abstract =   { Learning in Gaussian Process models occurs through the adaptation of hyperparameters of the mean and the covariance function. The classical approach entails maximizing the marginal likelihood yielding fixed point estimates (an approach called Type II maximum likelihood or ML-II). An alternative learning procedure is to infer the posterior over hyper-parameters in a hierarchical specication of GPs we call Fully Bayesian Gaussian Process Regression (GPR). This work considers two approximation schemes for the intractable hyperparameter posterior: 1) Hamiltonian Monte Carlo (HMC) yielding a sampling based approximation and 2) Variational Inference (VI) where the posterior over hyperparameters is approximated by a factorized Gaussian (mean-field) or a full-rank Gaussian accounting for correlations between hyperparameters. We analyse the predictive performance for fully Bayesian GPR on a range of benchmark data sets.}
}


@INPROCEEDINGS{Osborn,  author={Osborne, M. A. and Roberts, S. J. and Rogers, A. and Ramchurn, S. D. and Jennings, N. R.},  booktitle={2008 International Conference on Information Processing in Sensor Networks (ipsn 2008)},   title={Towards Real-Time Information Processing of Sensor Network Data Using Computationally Efficient Multi-output Gaussian Processes},   year={2008},  volume={},  number={},  pages={109-120},  doi={10.1109/IPSN.2008.25}}

@misc{kanagawa2018,
  doi = {10.48550/ARXIV.1807.02582},
  
  url = {https://arxiv.org/abs/1807.02582},
  
  author = {Kanagawa, Motonobu and Hennig, Philipp and Sejdinovic, Dino and Sriperumbudur, Bharath K},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{williams2006gaussian,
  title={Gaussian processes for machine learning},
  author={Williams, Christopher K and Rasmussen, Carl Edward},
  volume={2},
  number={3},
  year={2006},
  publisher={MIT press Cambridge, MA}
}

@InProceedings{lineBO,
  title =    {Adaptive and Safe {B}ayesian Optimization in High Dimensions via One-Dimensional Subspaces},
  author =       {Kirschner, Johannes and Mutny, Mojmir and Hiller, Nicole and Ischebeck, Rasmus and Krause, Andreas},
  booktitle =    {Proceedings of the 36th International Conference on Machine Learning},
  pages =    {3429--3438},
  year =   {2019},
  editor =   {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume =   {97},
  series =   {Proceedings of Machine Learning Research},
  month =    {09--15 Jun},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v97/kirschner19a/kirschner19a.pdf},
  url =    {https://proceedings.mlr.press/v97/kirschner19a.html},
  abstract =   {Bayesian optimization is known to be difficult to scale to high dimensions, because the acquisition step requires solving a non-convex optimization problem in the same search space. In order to scale the method and keep its benefits, we propose an algorithm (LineBO) that restricts the problem to a sequence of iteratively chosen one-dimensional sub-problems that can be solved efficiently. We show that our algorithm converges globally and obtains a fast local rate when the function is strongly convex. Further, if the objective has an invariant subspace, our method automatically adapts to the effective dimension without changing the algorithm. When combined with the SafeOpt algorithm to solve the sub-problems, we obtain the first safe Bayesian optimization algorithm with theoretical guarantees applicable in high-dimensional settings. We evaluate our method on multiple synthetic benchmarks, where we obtain competitive performance. Further, we deploy our algorithm to optimize the beam intensity of the Swiss Free Electron Laser with up to 40 parameters while satisfying safe operation constraints.}
}


@InProceedings{safeopt1,
  title =    {Safe Exploration for Optimization with Gaussian Processes},
  author =   {Sui, Yanan and Gotovos, Alkis and Burdick, Joel and Krause, Andreas},
  booktitle =    {Proceedings of the 32nd International Conference on Machine Learning},
  pages =    {997--1005},
  year =   {2015},
  editor =   {Bach, Francis and Blei, David},
  volume =   {37},
  series =   {Proceedings of Machine Learning Research},
  address =    {Lille, France},
  month =    {07--09 Jul},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v37/sui15.pdf},
  url =    {https://proceedings.mlr.press/v37/sui15.html},
  abstract =   {We consider sequential decision problems under uncertainty, where we seek to optimize an unknown function from noisy samples. This requires balancing exploration (learning about the objective) and exploitation (localizing the maximum), a problem well-studied in the multi-armed bandit literature. In many applications, however, we require that the sampled function values exceed some prespecified "safety" threshold, a requirement that existing algorithms fail to meet. Examples include medical applications where patient comfort must be guaranteed, recommender systems aiming to avoid user dissatisfaction, and robotic control, where one seeks to avoid controls causing physical harm to the platform. We tackle this novel, yet rich, set of problems under the assumption that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop an efficient algorithm called SafeOpt, and theoretically guarantee its convergence to a natural notion of optimum reachable under safety constraints. We evaluate SafeOpt on synthetic data, as well as two real applications: movie recommendation, and therapeutic spinal cord stimulation.}
}

@Article{safeopt2,
author={Berkenkamp, Felix
and Krause, Andreas
and Schoellig, Angela P.},
title={Bayesian optimization with safety constraints: safe and automatic parameter tuning in robotics},
journal={Machine Learning},
year={2021},
month={Jun},
day={24},
abstract={Selecting the right tuning parameters for algorithms is a pravelent problem in machine learning that can significantly affect the performance of algorithms. Data-efficient optimization algorithms, such as Bayesian optimization, have been used to automate this process. During experiments on real-world systems such as robotic platforms these methods can evaluate unsafe parameters that lead to safety-critical system failures and can destroy the system. Recently, a safe Bayesian optimization algorithm, called SafeOpt, has been developed, which guarantees that the performance of the system never falls below a critical value; that is, safety is defined based on the performance function. However, coupling performance and safety is often not desirable in practice, since they are often opposing objectives. In this paper, we present a generalized algorithm that allows for multiple safety constraints separate from the objective. Given an initial set of safe parameters, the algorithm maximizes performance but only evaluates parameters that satisfy safety for all constraints with high probability. To this end, it carefully explores the parameter space by exploiting regularity assumptions in terms of a Gaussian process prior. Moreover, we show how context variables can be used to safely transfer knowledge to new situations and tasks. We provide a theoretical analysis and demonstrate that the proposed algorithm enables fast, automatic, and safe optimization of tuning parameters in experiments on a quadrotor vehicle.},
issn={1573-0565},
doi={10.1007/s10994-021-06019-1},
url={https://doi.org/10.1007/s10994-021-06019-1}
}



@INPROCEEDINGS{safeoptberkenkamp,  author={Berkenkamp, Felix and Schoellig, Angela P. and Krause, Andreas},  booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)},   title={Safe controller optimization for quadrotors with Gaussian processes},   year={2016},  volume={},  number={},  pages={491-496},  doi={10.1109/ICRA.2016.7487170}}

@phdthesis{heuer,
  author = {Michael Heuer},
  title = {Identification and control of the laser-based synchronization system for the European X-ray Free Electron Laser},
  year = {2018},
        copyright = {http://rightsstatements.org/vocab/InC/1.0/},
        school = {Technische Universität Hamburg-Harburg},
        type = {doctoralThesis},
  abstract = {(wird nachgereicht)},
  abstract = {The European X-ray Free-Electron Laser is currently under construction at theDeutsches ElektronenSynchrotron in Hamburg, Germany. This linear accelerator, with a length of 3.4 km,will generate extremely intense and short X-ray laser light pulses with a duration in the femto-second range and wavelengths down to 0.05 nm. These laser pulses provide physicists with alight source to take a closer look into small structures on atomic scale.Those precise measurements require timing with an error margin in the femto-second range for most subsystems within the facility. Usually, this timing signal is distributed electrically via coaxial cables. With the new requirements in timing, this kind of distribution is no longer suitable and a new laser-based synchronization system is used. This system generates a laser pulse train via a master laser oscillator and distributes this via optical fiber to multiple endstations in the facility.The effective length of the optical path inside the fiber is actively stabilized by a link stabilizing unit.This thesis analyzes this new system from a control point of view. It is shown that the master laser oscillator can be modeled by an integrator, with the H2 norm as the performance criteria and two filters corresponding to the noise and disturbances of the master laser oscillator itself as well as the electrical oscillator of the facility. Those influences, as well as the dynamic behavior of the master laser oscillator, are identified for a laboratory setup. With these models in hand, different controllers are designed and experimentally evaluated. A sufficient controller performance can be achieved by a PI controller. However, using a feedback controller with a model-based optimization increase this performance, but these require a high order of the controller, which is currently not implementable given the installed hardware.The second part of this work analyses the link stabilizing units. This is achieved with an attached optical fiber and a timing measurement by an optical cross correlator. If a short optical fiber is connected the system can be approximated by a third order system with a time delay of a few sample. A model is identified and used for controller design. It can be shown that a performance increase by factor of 4.5 can be achieved if an LQG controller, including a model of the time delay, is used instead of the previously used PI controller. Moreover, different approaches for long optical fibers and the operation in the non-linear region of the sensor are shown. These could not be tested in an experiment.The work closes with an analysis of the overall system and gives suggestions of how to increase the performance of the individual components and of the whole laser-based synchronization system including the attached devices. It will be shown that the optimal performance can be achieved if all systems are connected to the laser-based synchronization system and if the dynamic behavior of the link stabilizing unit and end-station is equal for all subsystems.},
  url = {http://tubdok.tub.tuhh.de/handle/11420/1706},
  doi = {10.15480/882.1703},  
}

@INPROCEEDINGS{whitebox_max,
  author={Schütte, Maximilian and Eichler, Annika and Schlarb, Holger and Lichtenberg, Gerwald and Werner, Herbert},
  booktitle={2021 60th IEEE Conference on Decision and Control (CDC)},
  title={Decentralized Output Feedback Control using Sparsity Invariance with Application to Synchronization at European XFEL},
  year={2021},
  volume={},
  number={},
  pages={5723-5728},
  doi={10.1109/CDC45484.2021.9683027}
  }

@Article{Moriconi2020,
author={Moriconi, Riccardo
and Deisenroth, Marc Peter
and Sesh Kumar, K. S.},
title={High-dimensional Bayesian optimization using low-dimensional feature spaces},
journal={Machine Learning},
year={2020},
month={Sep},
day={01},
volume={109},
number={9},
pages={1925-1943},
abstract={Bayesian optimization (BO) is a powerful approach for seeking the global optimum of expensive black-box functions and has proven successful for fine tuning hyper-parameters of machine learning models. However, BO is practically limited to optimizing 10--20 parameters. To scale BO to high dimensions, we usually make structural assumptions on the decomposition of the objective and/or exploit the intrinsic lower dimensionality of the problem, e.g. by using linear projections. We could achieve a higher compression rate with nonlinear projections, but learning these nonlinear embeddings typically requires much data. This contradicts the BO objective of a relatively small evaluation budget. To address this challenge, we propose to learn a low-dimensional feature space jointly with (a) the response surface and (b) a reconstruction mapping. Our approach allows for optimization of BO's acquisition function in the lower-dimensional subspace, which significantly simplifies the optimization problem. We reconstruct the original parameter space from the lower-dimensional subspace for evaluating the black-box function. For meaningful exploration, we solve a constrained optimization problem.},
issn={1573-0565},
doi={10.1007/s10994-020-05899-z},
url={https://doi.org/10.1007/s10994-020-05899-z}
}



@Article{EI,
author={Jones, Donald R.
and Schonlau, Matthias
and Welch, William J.},
title={Efficient Global Optimization of Expensive Black-Box Functions},
journal={Journal of Global Optimization},
year={1998},
month={Dec},
day={01},
volume={13},
number={4},
pages={455-492},
abstract={In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
issn={1573-2916},
doi={10.1023/A:1008306431147},
url={https://doi.org/10.1023/A:1008306431147}
}

@article{EI2,
  title={The application of Bayesian methods for seeking the extremum},
  author={Mockus, Jonas and Tiesis, Vytautas and Zilinskas, Antanas},
  journal={Towards global optimization},
  volume={2},
  number={117-129},
  pages={2},
  year={1978}
}

@misc{tutorial_bayesopt,
  doi = {10.48550/ARXIV.1807.02811},
  
  url = {https://arxiv.org/abs/1807.02811},
  
  author = {Frazier, Peter I.},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {A Tutorial on Bayesian Optimization},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{tutorial_bayesopt_brochu,
  doi = {10.48550/ARXIV.1012.2599},
  
  url = {https://arxiv.org/abs/1012.2599},
  
  author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences, G.1.6; G.3; I.2.6},
  
  title = {A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2010},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{wahba1990spline,
  title={Spline Models for Observational Data},
  author={Wahba, Grace},
  volume={59},
  year={1990},
  publisher={SIAM}
}

@INPROCEEDINGS{sasbo,
  author={Blasi, Stefano De and Gepperth, Alexander},
  booktitle={2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
  title={SASBO: Self-Adapting Safe Bayesian Optimization}, 
  year={2020},
  volume={},
  number={},
  pages={220-225},
  doi={10.1109/ICMLA51294.2020.00044}}
  
  @article{safeBO3,
title = {Safe Active Learning and Safe Bayesian Optimization for Tuning a PI-Controller},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {5967-5972},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.1258},
url = {https://www.sciencedirect.com/science/article/pii/S240589631731772X},
author = {Mark Schillinger and Benjamin Hartmann and Patric Skalecki and Mona Meister and Duy Nguyen-Tuong and Oliver Nelles},
keywords = {engine modelling, control, modeling, supervision, control, diagnosis of automotive systems},
abstract = {In the combustion engine calibration domain, many controllers are still tuned manually or using simple adjustment laws. In order to increase workforce efficiency, automated methods for controller tuning are desirable. Often, the structure of the controller is fixed and only its parameters have to be optimized. Model based controller tuning methods require a good dynamic model of the system. Such models are often hard to obtain, as deep system knowledge or extensive measurements at the system, potentially in open loop, may be required. In some cases only closed loop measurements are possible, for example, due to system instability. If controller tuning methods interact with the real system for which the controller shall be tuned, they have to comply with safety constraints. For example, parameter sets resulting in unstable control loops or ones causing critical system states as with very high overshoot should not be tested at the real system. In this contribution, two optimization-based methods for tuning controller parameters are compared. The first method, Safe Active Learning for control, learns a loss function based on controller parameters. Subsequently, an offline optimization is pursued. The second method, a newly proposed Safe Bayesian Optimization algorithm, combines learning of a loss function model with online optimization. Both methods perform closed loop measurements and take safety constraints into account. The methods are evaluated and compared at a PI controller of a real high pressure fuel supply system in a test vehicle.}
}

@INPROCEEDINGS{heat_pump_annika,
  author={Khosravi, Mohammad and Eichler, Annika and Schmid, Nicolas and Smith, Roy S. and Heer, Philipp},
  booktitle={2019 18th European Control Conference (ECC)}, 
  title={Controller Tuning by Bayesian Optimization An Application to a Heat Pump}, 
  year={2019},
  volume={},
  number={},
  pages={1467-1472},
  doi={10.23919/ECC.2019.8795801}}

@book{mockus2012bayesian,
  title={Bayesian approach to global optimization: theory and applications},
  author={Mockus, Jonas},
  volume={37},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@misc{stageopt,
  doi = {10.48550/ARXIV.1806.07555},
  url = {https://arxiv.org/abs/1806.07555},
  author = {Sui, Yanan and Zhuang, Vincent and Burdick, Joel W. and Yue, Yisong},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Stagewise Safe Bayesian Optimization with Gaussian Processes},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{contextual_safepid_tuning,
  doi = {10.48550/ARXIV.1906.12086},
  
  url = {https://arxiv.org/abs/1906.12086},
  
  author = {Fiducioso, Marcello and Curi, Sebastian and Schumacher, Benedikt and Gwerder, Markus and Krause, Andreas},
  
  keywords = {Machine Learning (cs.LG), Systems and Control (eess.SY), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Safe Contextual Bayesian Optimization for Sustainable Room Temperature PID Control Tuning},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{meta_learning_priors,
  doi = {10.48550/ARXIV.2210.00762},
  
  url = {https://arxiv.org/abs/2210.00762},
  
  author = {Rothfuss, Jonas and Koenig, Christopher and Rupenyan, Alisa and Krause, Andreas},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Robotics (cs.RO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Meta-Learning Priors for Safe Bayesian Optimization},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@article{heat_pump2,
author={Khosravi,Mohammad and Schmid,Nicolas and Eichler,Annika and Heer,Philipp and Smith,Roy S.},
year={2019},
month={11},
title={Machine learning-based modeling and controller tuning of a heat pump},
journal={Journal of Physics: Conference Series},
volume={1343},
number={1},
note={Copyright - © 2019. This work is published under http://creativecommons.org/licenses/by/3.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License; Last updated - 2021-09-01},
abstract={In this paper, we consider the problem of controller tuning for an operating unit in a building energy system. The illustrative example used here is a real heat pump located in the NEST building at Empa, Dubendorf, Zurich, with its outflow temperature controlled by a PI-controller. The plant is in use and accordingly, intervening in its normal operation is not allowed. Moreover, the model of plant is not available or it can be changed due to aging or possible modification. Accordingly, it is desired to utilize a tuning method which is model-free, operates online, does not intervene with the normal operation of the plant and use only the available historical measurement data. Additionally, it is required to guarantee the safety of the plant during the tuning procedure. In this regard, we formulate the controller tuning problem as a black-box optimization and adopt a safe Bayesian optimization approach for controller parameters tuning. In order to assess numerically the performances of the scheme, first we model the plant as a nonlinear ARX model in form of a feedforward neural network. Subsequently, we train the neural network using the available historical measurement data. Finally, the obtained model is used as an oracle in the controller tuning procedure in order to numerically verify the effectivity of the proposed approach.},
keywords={Physics; Mathematical models; Tuning; Machine learning; Heat pumps; Controllers; Artificial neural networks; Neural networks; Optimization},
isbn={17426588},
language={English},
url={https://www.proquest.com/scholarly-journals/machine-learning-based-modeling-controller-tuning/docview/2568054342/se-2},
} 

@misc{highdim_bayes_dropout,
  doi = {10.48550/ARXIV.1802.05400},
  
  url = {https://arxiv.org/abs/1802.05400},
  
  author = {Li, Cheng and Gupta, Sunil and Rana, Santu and Nguyen, Vu and Venkatesh, Svetha and Shilton, Alistair},
  
  keywords = {Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {High Dimensional Bayesian Optimization Using Dropout},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}